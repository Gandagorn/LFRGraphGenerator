"""
This file includes functions for generating and modifying basic LFR graphs and 
for LFR graphs with sensitive features.
"""

import copy
import multiprocessing
import time
import itertools
from itertools import combinations
from numpy.random import multivariate_normal
from random import choice

import networkx as nx
import numpy as np
import pandas as pd
import torch
from community import community_louvain
from networkx.algorithms.community.quality import modularity
from networkx.generators.community import LFR_benchmark_graph
from torch_geometric.data import Data
from tqdm import tqdm

torch.set_num_threads(1)


def generate_LFR_graph(
    n, mu, avg_degree, feature_dim=5, feature_cov=1, tau1=3, tau2=1.1, 
    min_community_size=100, seed=42, tol=1e-7, num_iters=2000
    ):
    """Generate a LFR graph according to parameters using the networkx 
    implementation of LFR.
    Features are computed from overlapping gaussians with means on the corners 
    of a hypercube with the same dimensionality as the features.

    Parameters
    ----------
    n : int
        Number of nodes

    mu : float
        Fraction of intra-community edges

    avg_degree : float
        Desired average degree of nodes in the created graph

    feature_dim : int, optional
        Dimension of features, by default 5

    feature_cov : float, optional
        Diagonal values of covariance matrix of feature gaussians. A 
        higher value gives more overlapping gaussians and makes predictions 
        harder, by default 1

    tau1 : float, optional
        Power-law exponent for the degree distribution, by default 3

    tau2 : float, optional
        Power-law exponent for the community size distribution, by default 1.1

    min_community_size : int, optional
        Minimum size of communities in the graph. A too small 
        size can make it hard to generate a LFR graph, by default 100

    seed : int, optional
        Seed for LFR algorithm, by default 42

    tol : float, optional
        Tolerance when comparing floats, specifically when comparing average 
        degree values, by default 1e-7

    num_iters : int, optional
        Maximum number of iterations to try to create the community sizes, 
        degree distribution, and community affiliations, by default 1000

    Returns
    -------
    torch_geometric data
        Resulting LFR graph in pytorch geometric format
    """

    G = LFR_benchmark_graph(
            n=n, tau1=tau1, tau2=tau2, mu=mu, average_degree=avg_degree, 
            min_community=min_community_size, seed=seed, max_iters=num_iters, 
            tol=tol
    )

    labels = get_graph_communities_from_LFR(G)
    label_dict = {i: y.item() for i, y in enumerate(labels)}
    nx.set_node_attributes(G, label_dict, "y")

    G = connect_components(G)

    data, G = dataset_from_LFR(G, dimension=feature_dim, cov_diag=feature_cov)

    return data


def connect_components(G):
    """Connects separate components of LFR graph with random nodes from main 
    component

    Parameters
    ----------
    G : networkx graph
        Graph object

    Returns
    -------
    networkx graph
        Connected graph
    """    

    connected_components = list(nx.connected_components(G))

    ind_biggest_component = np.argmax(map(len, connected_components))
    biggest_component = connected_components.pop(ind_biggest_component)

    for comp in connected_components:
        G.add_edge( choice(tuple(comp)), choice(tuple(biggest_component)) )
    
    return G


def get_graph_communities_from_LFR(G):
    """Helper function to extract communities generated by LFR

    Parameters
    ----------
    G : networkx graph
        LFR graph

    Returns
    -------
    np.array
        Array of node communities
    """

    communities = {frozenset(G.nodes[v]["community"]) for v in G}

    node_communities = np.zeros(G.number_of_nodes())
    for i, comm in enumerate(communities):
        members = np.array(list(comm))
        node_communities[members] = i

    return node_communities


def dataset_from_LFR(G, dimension=3, cov_diag=0.5):
    """Transform LFR graph into pytorch data and create artificial features for 
    each class. Features are computed from overlapping Gaussian distributions, 
    each centered on the corners of a hypercube, which has the same 
    dimensionality as there are features.

    Parameters
    ----------
    G : networkx graph
        Graph object

    dimension : int, optional
        Dimension of the hypercube and number of features, by default 3

    cov_diag : float, optional
        Value of diagonal of covariance matrix to control the overlap of the
        Gaussians, by default 0.5

    Returns
    -------
    tuple
        first element is the graph as a pytorch data object, second element as
        a networkx object
    """

    labels = np.array(list(nx.get_node_attributes(G, "y").values()))
    n = len(labels)

    means = list(itertools.product([1, -1], repeat=dimension))
    unique_labels = np.unique(labels)
    num_labels = len(unique_labels)

    features = np.zeros((len(labels), dimension))
    for label, mean in zip(unique_labels, means[:num_labels]):

        num_features_per_label = sum(labels == label)
        cov = np.diag([cov_diag] * dimension) # defines separability of classes
        sample = multivariate_normal(mean, cov, num_features_per_label)
        features[labels == label] = sample

    feature_dict = {i: np.array(x) for i, x in enumerate(features)}
    nx.set_node_attributes(G, feature_dict, "x")

    # define number of nodes in each set
    val_size = int(n * 0.25)
    test_size = int(n * 0.25)
    train_size = int(n - (val_size + test_size))

    data = Data()
    data["x"] = torch.from_numpy(features).float()
    data["y"] = torch.tensor(labels).long()
    data["edge_index"] = torch.tensor(np.array(G.edges()).T)
    data["train_mask"] = torch.tensor([True]*train_size + [False]*val_size + [False]*test_size)
    data["val_mask"] = torch.tensor([False]*train_size + [True]*val_size + [False]*test_size)
    data["test_mask"] = torch.tensor([False]*train_size + [False]*val_size + [True]*test_size)
    
    return data, G


def data_to_nx(data, keep_direction=False, remove_selfloops=False):
    """Converts graph data to networkx graph.
    Optionally keep directionality and self-loops.

    Parameters
    ----------
    data : torch_geometric data
        Graph data

    keep_direction : bool, optional
        If True directions are taken into account, by default False

    remove_selfloops : bool, optional
        If True, self-loops are removed, by default False

    Returns
    -------
    G : networkx graph
        Graph object
    """    

    if keep_direction:
        G = nx.DiGraph()
    else:
        G = nx.Graph()

    G.add_nodes_from(np.arange(len(data["y"])))

    edges = np.array(data["edge_index"]).T
    labels = np.array(data["y"])

    G.add_edges_from(edges)

    if remove_selfloops:
        G.remove_edges_from(nx.selfloop_edges(G))

    feature_dict = {i: np.array(x) for i, x in enumerate(data["x"])}
    label_dict = {i: y.item() for i, y in enumerate(data["y"])}
    nx.set_node_attributes(G, feature_dict, "x")
    nx.set_node_attributes(G, label_dict, "y")

    return G


def try_graph_with_params(res, *args, **kwargs):
    """Helper function for multiprocessing

    Parameters
    ----------
    res : multiprocessing.managers.ListProxy
        Stores resulting graphs of processes using different seed values
    """    

    graph = generate_LFR_graph(*args, **kwargs)
    res.append((graph, kwargs["seed"]))


def search_graph_parallel(num_to_try=6, timeout=20, *args, **kwargs):
    """Tries to generate several graphs with graph parameters in parallel and 
    returns the first found graph with its corresponding seed.

    Parameters
    ----------
    num_to_try : int, optional
        Number of different seed values to be evaluated in parallel, 
        by default 4

    timeout : int, optional
        Maximum time (s) after which processing is stopped, by default 20

    Returns
    -------
    tuple
        Return found LFR graph and corresponding seed to recreate graph

    Raises
    ------
    RuntimeError
        If no graph can be found with given parameters.

    Example Usage
    ------
    found_d, seed = search_graph_parallel( 
                    n=19717, 
                    mu=0.2,
                    avg_degree=3.9 
    )
    """    

    manager = multiprocessing.Manager()
    res = manager.list()

    processes = []
    for i in range(num_to_try):
        kwargs["seed"] = np.random.randint(0, 1_000)
        tst = multiprocessing.Process(
            target=try_graph_with_params, args=(res, *args), kwargs=kwargs, 
            daemon=True
        )
        tst.start()
        processes.append(tst)

    for i in range(timeout):
        if len(res) == 0:
            time.sleep(1)
            continue
        else:
            break

    for p in processes:
        p.terminate()

    for d in res:
        if d != None:
            return d

    raise RuntimeError(
        f"""Can't find working graph with parameters {args} {kwargs}.
        This does not necessarily mean that there are no working seeds with the 
        paramters.
        """)


def generate_mixed_graph(mus, avg_degrees, n, seeds=None):
    """Creates several graphs according to parameters and mixes them.
    Length of mu and degree values specifiy the number of graphs.
    In each returned graph, only the labels differ, as each label set comes from
    a single generated LFR graph.

    Parameters
    ----------
    mus : list
        List of mu values (mixing parameters) for LFR generation

    avg_degrees : list
        List of average degrees for LFR generation

    n : int
        Number of nodes

    seeds : int, optional
        List of seeds to be used for graph generation. If no seeds are provided, 
        new ones are generated, by default None

    Returns
    -------
    list
        list of pytorch geometric data objects containing the resulting graphs
    """
    
    # sample graphs
    data_list = []
    if seeds == None:
        for mu, avg_degree in tqdm(zip(mus, avg_degrees)):
            d, _ = search_graph_parallel(
                        n=n, 
                        mu=mu,
                        avg_degree=avg_degree, 
            )
            
            data_list.append(d)

    else:
        for mu, avg_degree, seed in tqdm(zip(mus, avg_degrees, seeds)):
            d = generate_LFR_graph(
                        n=n, 
                        mu=mu,
                        avg_degree=avg_degree, 
                        seed=seed,
            )
            
            data_list.append(d)

    # combine edges
    G = nx.Graph()
    for d in data_list:
        G.add_edges_from(np.array(d.edge_index).T)
    edges = torch.tensor(np.array(G.edges).T)
    
    # combine features
    features = torch.hstack([g.x for g in data_list])

    # update graphs
    for d in data_list:
        d.edge_index = edges
        d.x = features

    return data_list


def combine_2_comms(G, pre_partition, verbose=False):
    """Given a Graph and a partition in communities, this function combines the 
    two communities that maximize the modularity of the resulting partition.
    If the partition is already optimial, the new partition will have a 
    decreased modularity.

    Parameters
    ----------
    G : networkx graph
        Graph object

    pre_partition : dict
        Contains original partition

    verbose : bool, optional
        by default False

    Returns
    -------
    dict
        new partition
    """
        
    comms = np.unique(list(pre_partition.values()))

    max_mod = -np.inf
    best_partition = None
    for fc, sc in list(combinations(comms, 2)):
        mod_partition = [set() for _ in range(len(comms)) ]

        for n, c in pre_partition.items():
            if c == sc:
                mod_partition[fc].add(n) # assign second community to first community
            else:
                mod_partition[c].add(n)

        mod = modularity(G, mod_partition)

        if mod > max_mod:
            max_mod = mod
            best_partition = mod_partition

    if verbose:
        print(modularity(G, best_partition))
        print([len(x) for x in best_partition])

    best_partition_dict = dict()
    
    # remove empty communities
    i = 0
    for n_set in best_partition:

        if len(n_set) == 0:
            continue

        for n in n_set:
            best_partition_dict[n] = i
        
        i += 1

    return best_partition_dict


def get_reduced_partition(G, num_comms=2, verbose=False):
    """Given a graph, compute a partition with a desired number of communities.
    
    First the graph is partitioned using the louvaine algorithm. 
    The communities of the louvaine algorithm are then combined in pairs until 
    the desired number of communities is reached.
    The pairs to be combined are based on modularity.

    Parameters
    ----------
    G : networkx graph
        Graph object to be  partitioned

    num_comms : int, optional
        Number of communities in the resulting partition, by default 2

    verbose : bool, optional
        by default False

    Returns
    -------
    tuple
        Tuple containing the start partition obtained by the louvaine algorithm
        and the resulting partition with the desired number of communities
    """

    start_partition = community_louvain.best_partition(G)

    current_comms = len(np.unique(list(start_partition.values())))

    # starting modularity
    mod_partition = [set() for _ in range(current_comms)]
    for n, c in start_partition.items(): mod_partition[c].add(n)

    if verbose:
        print(current_comms)
        print(f"starting modularity {modularity(G, mod_partition):.2f}")

    tmp_partition = start_partition
    for _ in range(np.max([current_comms - num_comms, 0])):
        tmp_partition = combine_2_comms(G, tmp_partition)

    return start_partition, tmp_partition


def combine_subcommunities(data, num_comms, sensitive_bias=0.):
    """Combines the sub-communities to a desired number of communities and 
    generates 2 additional graphs with a sensitive variable.

    In the first graph the binary variable has a random distribution.
    In the second graph, the binary variable follows a different distribution in 
    each of the original sub-communities given by louvaine.
    For each sub-community a probability is sampled from a uniform distribution, 
    which denotes the probability of the sensitive variable being True in the 
    sub-community.

    The sensitive_bias parameter can be used to make the probability more 
    extreme (more close to 0 or 1). 
    For probabilites <0.5 the new probability is p_new = p - sensitive_bias, and 
    for >0.5 the new probability is p_new = p + sensitive_bias.
    With sensitive_bias = 0.5, the resulting probabilites will be either 1 or 0, 
    which result in the same value of the sensitive variable for all nodes in a 
    sub-community.

    Parameters
    ----------
    data : torch_geometric data
        Contains the graph data

    num_comms : int
        Number of communities in the resulting partition
        
    sensitive_bias : float, optional
        Bias to make the sub-community probabilites more extrem, by default 0

    Returns
    -------
    tuple
        Pytorch geometric data object containing the graph with reduced 
        communities and 2 graphs with a random and a controlled sensitive 
        variable
    """

    # make reduced partition
    def get_labels_from_label_dict(label_dict):
        labels = np.zeros(len(label_dict))
        for k, v in label_dict.items():
            labels[k] = v

        return labels

    reduced_data = copy.deepcopy(data)

    start_partition, reduced_partition = get_reduced_partition(data_to_nx(data), num_comms=num_comms)
    labels = get_labels_from_label_dict(reduced_partition)

    reduced_data["y"] = torch.tensor(labels).long()

    # make sensitive variable
    label_df = pd.DataFrame({
        "start_comms": get_labels_from_label_dict(start_partition), 
        "comb_comms": get_labels_from_label_dict(reduced_partition)
    })

    merged_comms_mapping = label_df.groupby("comb_comms").agg(lambda x: list(np.unique(x)))

    label_df["sensitive_controlled"] = np.nan

    for row in merged_comms_mapping.iterrows():
        start_comms = row[0]
        merged_comms = row[1][0]

        filt_label_df = label_df[label_df["start_comms"].isin(merged_comms)]
        filt_label_df_counts = filt_label_df["start_comms"].value_counts()

        sum_nodes = len(filt_label_df)

        pre_assigned_correlations = np.random.random(len(merged_comms))
        pre_assigned_correlations[pre_assigned_correlations<0.5] = pre_assigned_correlations[pre_assigned_correlations<0.5] - sensitive_bias
        pre_assigned_correlations[pre_assigned_correlations>0.5] = pre_assigned_correlations[pre_assigned_correlations>0.5] + sensitive_bias
        pre_assigned_correlations = np.clip(pre_assigned_correlations, 0, 1)

        for (comm, num_nodes), corr in zip(filt_label_df_counts.iteritems(), pre_assigned_correlations):

            num_comm_nodes = filt_label_df_counts[comm]
            label_df.loc[label_df["start_comms"]==comm, "sensitive_controlled"] = np.random.choice([0, 1], size=num_comm_nodes, p=[corr, 1-corr])

    label_df["sensitive_random"] = np.random.choice([0,1], len(label_df), p=[0.5, 0.5])

    reduced_data_sensitive_controlled = copy.deepcopy(reduced_data)
    reduced_data_sensitive_controlled.x = torch.from_numpy(np.hstack([reduced_data.x, label_df["sensitive_controlled"].values.reshape(-1,1)])).float()

    reduced_data_sensitive_random = copy.deepcopy(reduced_data)
    reduced_data_sensitive_random.x = torch.from_numpy(np.hstack([reduced_data.x, label_df["sensitive_random"].values.reshape(-1,1)])).float()

    return reduced_data, reduced_data_sensitive_controlled, reduced_data_sensitive_random, label_df